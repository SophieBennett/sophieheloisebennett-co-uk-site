---
title: "Why Linear Regression Estimates the Conditional Mean"
author: "Sophie Bennett"
date: "2020-05-27"
output: html_document
---

<style>
  pre.r { 
    color:#000;
    background-color:#ECECEC;
  }
</style>

```{r setup, include=FALSE, error=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

library(ggplot2)
library(magrittr)
extrafont::loadfonts(device="win")

```

Because you can never know too much about linear regression.

## Introduction

If you look at any textbook on linear regression, you will find that it says the following:

"Linear regression estimates the **conditional mean** of the response variable."

This means that, for a given value of the predictor variable $X$, linear regression will give you the **mean** value of the response variable $Y$. 

Now, in and of itself, it's a pretty neat fact... but, *why* is it true? 

Like me, you may have been tempted to take to google for an answer. And, like me, you *may* have found the online explanations hard to follow.

This is my attempt to break down the explanation more simply.

## Recap on Linear Regression 

Let's begin with a quick recap on linear regression. 

In (simple) linear regression, we are looking for a line of best fit to model the relationship between our predictor, $X$ and our response variable $Y$.

The line of best fit takes the form of an equation

$$Y = \beta_{0} + \beta_{1}X$$

where $\beta_{0}$ is the intercept, and $\beta_{1}$ is the coefficient of the slope.

To find the intercept and slope coefficients of the line of best fit, linear regression uses the least squares method, which seeks to minimise the sum of squared deviations between the $n$  **observed** data points $y_{1}...y_{n}$ and the **predicted** values, which we'll call $\hat{y}$:

$$\sum_{i=1}^{n} (y_{i} - \hat{y})^2$$

And, it turns out that the line of best fit obtained with the least squares method *always* gives us the conditional mean of the response variable $Y$. 

Now, this kinda answers our original question... but it also leads to a new question: why is minimising the sum of squared deviations equivalent to finding the conditional mean?

The simple answer is that it can be proved mathematically that minimising the sum of squared deviations always gives you the conditional mean of the response variable. 

Let's now look at how that works.

## The Sum of Squared Deviations Method

So far, we've talked about minimising the sum of squared deviations in the context of linear regression. However, we can also calculate a sum of squared deviations in other contexts too.

For instance, let's generate a dataset of 1000 numbers, with a mean of ~20 and a standard deviation of 2.

<div class="highlight">
```{r}

set.seed(8825)
sample_data <- rnorm(1000, mean = 20, sd = 2)

# confirm mean is ~= 20
mean(sample_data) 

```
</div> 

Now, we could calculate the sum of the squared deviations of each of these numbers from the mean...

<div class="highlight">
```{r}

sum((sample_data - mean(sample_data))^2)

```
</div>

...(which is exactly what we'd need to do to calculate the standard deviation of the set of numbers)

And we could also calculate the sum of the squared deviations of these numbers from any other value, such as the median, mode, or any other arbitrary value.

So now, let's calculate the sum of the squared deviations using a variety of different values: 

<div class="highlight">
```{r}

# values to calculate the deviation from in our dataset
dev_values <- c(0, 5, 10, 12, 15, 18, 19, 19.92, 21, 22, 25, 28, 30, 35, 40)

# generate empty list
squared_residuals <- rep(NA, length(dev_values))

# calculate sum of squared deviations of the data from each value in dev_values
for (i in 1:length(dev_values)) {
  
  squared_residuals[i] = sum((sample_data - dev_values[i])^2)
  
}

squared_residuals 

```
</div>
Next, let's plot the resulting sum of squared deviations obtained using each value:

<div class="highlight">
```{r, fig.align="center"}

data.frame(dev_values = dev_values, squared_residuals = squared_residuals) %>% 
  ggplot(aes(dev_values, squared_residuals)) +
  stat_smooth(method="lm", 
              formula = y ~ poly(x, 2), 
              se = FALSE,
              colour = "#FCC3B6",
              linetype = "dashed") +
  geom_point(col = "#C70039", size = 2.2, alpha = 0.7) +
  labs(title = "Sum of Squared Residuals (SSR) Loss Function",
     x = "Summary Value",
     y = "SSR") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"),
        plot.title = element_text(family = "Lato Semibold", hjust = 0.5)) +
  scale_y_continuous(labels = scales::comma)

```
</div>

Notice that the value that gives us the _smallest_ sum of squared deviations, the lowest point on our curve, turns out to be 19.92, which is the mean of our dataset!

Now, this isn't just a fun feature of our sample dataset; given any set of numbers $x_{1}...x_{n}$, the value that results in the smallest sum of squared deviations will *always* be the mean.

And just in the same way, in linear regression, the predicted $\hat{y}$ values that minimise the sum of squared deviations will always be the conditional mean of $y$.

This simulation might help you see how minimising the sum of squared deviations is equivalent to using the mean, but it still doesn't explain *why* it's the case.

For that, we need to look at the mathematical proof.

## Mathematical Proof: Background

When we calculate the sum of squared deviations between some sample data, and another value, what we're really doing is passing the data through a function:

$$f(y) = \sum_{i=1}^{n} (y_{i} - \hat{y})^2$$
And, in minimising the sum of squared deviations, our aim is to find the value for $\hat{y}$ that minimises the output of the function.

Now, whenever we have a function whose output we want to minimise, we call the function a **loss** function, denoted as $L(y)$.

So, we can write our sum of squared deviations function as this:

$$L(y) = \sum_{i=1}^{n} (y_{i} - \hat{y})^2$$

Whenever we want to find the value of $\hat{y}$ that **minimises** a loss function, the way to solve this problem is by differentiation.

Why? Well, let's take a look back at our plot, where we calculated the sum of squared deviations for different values of $\hat{y}$. 

```{r, fig.align="center", echo=FALSE}

data.frame(dev_values = dev_values, squared_residuals = squared_residuals) %>% 
  ggplot(aes(dev_values, squared_residuals)) +
  stat_smooth(method="lm", 
              formula = y ~ poly(x, 2), 
              se = FALSE,
              colour = "#FCC3B6",
              linetype = "dashed") +
  geom_point(col = "#C70039", size = 2.2, alpha = 0.7) +
  labs(title = "Sum of Squared Residuals (SSR) Loss Function",
     x = "Summary Value",
     y = "SSR") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"),
        plot.title = element_text(family = "Lato Semibold", hjust = 0.5)) +
  scale_y_continuous(labels = scales::comma)

```

The value that we want to find out, the one that minimises the sum of squared deviations, is the one at the lowest point of the curve, where the gradient of the curve is equal to zero.

```{r, fig.align="center", echo=FALSE}

data.frame(dev_values = dev_values, squared_residuals = squared_residuals) %>% 
  ggplot(aes(dev_values, squared_residuals)) +
  stat_smooth(method="lm", 
              formula = y ~ poly(x, 2), 
              se = FALSE,
              colour = "#FCC3B6",
              linetype = "dashed") +
  geom_point(col = "#C70039", size = 2.2, alpha = 0.7) +
  labs(title = "Sum of Squared Residuals (SSR) Loss Function",
     x = "Summary Value",
     y = "SSR") +
  geom_hline(yintercept = 0,
             colour = "gray75",
             size = 1,
             linetype = "longdash") +
  theme_minimal() +
  theme(text = element_text(family = "Lato"),
        plot.title = element_text(family = "Lato Semibold", hjust = 0.5)) +
  scale_y_continuous(labels = scales::comma)

```

And so, what we're really doing here is asking, what value does my summary statistic take, at the point at which the **gradient** of the sum of squared deviations function is equal to zero?

And finding gradients? Well, that's a job for differentiation!

So, we want to differentiate our loss function: 

$$\displaystyle \frac{d}{d\hat{y}} L(y) = \frac{dL}{d\hat{y}} \sum_{i=1}^{n} (y_{i} - \hat{y})^2$$
Differentiating the loss function gives us this:

$$\displaystyle \frac{dL}{d\hat{y}} = \sum_{i=1}^{n} -2(y_{i} - \hat{y})$$
It can be a little tricky to understand what's happened here, especially if you're not using to differentiations involving $\sum_{}$ symbols and $y_{i}$ terms.

To make it a bit clearer what I've just done, I'm going to momentarily pause on differentiating our actual loss function and instead detour toa simpler  problem, that of differentiating the equation $y = (1 - \hat{y}^{2})$: 

$$\displaystyle \frac{d}{d\hat{y}} L(y) = \frac{dL}{d\hat{y}}(1 - \hat{y})^2$$
Now, we can write this equation like so:

$$\displaystyle \frac{d}{d\hat{y}} L(y) = \frac{dL}{d\hat{y}}(1 - \hat{y})(1 - \hat{y})$$

Which, expanded out, gives us the following:

$$\displaystyle \frac{d}{d\hat{y}} L(y) = \frac{dL}{d\hat{y}}(1 - 2\hat{y} + \hat{y}^{2})$$

Finally, differentiating the above gives us this:

$$\displaystyle \frac{dL}{d\hat{y}} = (2\hat{y} - 2)$$
which is also equivalent to this:

$$\displaystyle \frac{dL}{d\hat{y}} = -2(1 - \hat{y})$$
And, the differentiation works pretty much the same way for our *actual* function, $L(y) = \sum_{i=1}^{n} -2(y_{i} - \hat{y})$:

$$\displaystyle \frac{dL}{d\hat{y}} = \sum_{i=1}^{n} -2(y_{i} - \hat{y})$$

Now, we want to find the value of $\hat{y}$ when the gradient is zero, so let's set this whole thing equal to zero:

$$\displaystyle 0 = \sum_{i=1}^{n} -2(y_{i} - \hat{y})$$

Divide both sides by -2 and we get this:

$$\displaystyle 0 = \sum_{i=1}^{n} (y_{i} - \hat{y})$$

Now, in the same way that $3(5- 4)$ is the same as $3 * 5 - 3 * 4$, the sum of $y_{i} - \hat{y}$ is the same as saying the sum of the $y_{i}$ values, minus the sum of adding up $\hat{y}$ n times:

$$\displaystyle 0 = \sum_{i=1}^{n} y_{i} - \sum_{i=1}^{n}\hat{y}$$

Which we can also write as:

$$\displaystyle 0 = \sum_{i=1}^{n} y_{i} - n\hat{y}$$

We want to find the value of $\hat{y}$, so let's rearrange the equation a little:

$$\displaystyle n\hat{y} = \sum_{i=1}^{n} y_{i}$$

Finally, let's divide both sides by n to find the value of $\hat{y}$.

$$\displaystyle \hat{y} = \frac{\sum_{i=1}^{n} y_{i}}{n}$$
And let's look at what we're left with here; $\hat{y}$ is equal to the sum of all the $y_{1}...y_{n}$ values in the data set, divided by $n$, the number of values in the dataset... otherwise knows as, the mean of the $y$ values!

Finally, I should point out that the proof I've gone through applies the sum of squares in the context of the mean a single set of values rather than in the context of minimising the sum of squares in linear regression. However, although the proof in the context of linear regression looks a bit more complicated, it works in exactly the same way.